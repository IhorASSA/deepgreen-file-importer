# -*- coding: utf-8 -*-
"""DeepGreen-DEMO-ver2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Jdn5fJaX8bfYvtfqhhoY89ETsFp7Rtfp

### Installation Project Libraries
"""

!pip install SQLAlchemy GeoAlchemy2 geopandas PyDrive psycopg2 --upgrade
!pip install geopandas oauth2client --upgrade

"""## Import Project Libraries

---


"""

import geopandas as gpd
import folium
import ftplib
import json
import logging as logger
import os
import numpy as np
import pandas as pd
import psutil
import requests
import time

from folium import Marker, GeoJson
from folium.plugins import HeatMap
from geopandas import GeoSeries
from google.colab import auth
from itertools import islice
from multiprocessing import Pool
from oauth2client.client import GoogleCredentials
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from shapely.geometry import Polygon, Point, MultiPolygon
from sqlalchemy import create_engine


logger.basicConfig(
    filename="log_file_test.log",
    filemode='a',
    format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',
    datefmt='%H:%M:%S',
    level=logger.DEBUG
)

logger.info("This is a test log ..")

"""## Constants and functions

### Define Constants
"""

EPSG = 3857
URL = 'https://lk.ukrforest.com/map/get-point-info?point=[{},{}]'

JSON_RESPONSE = {
    'index': '',
    'region': '',
    'district': '',
    'quarter': '',
    'square': '',
    'cutting_ticket_url': '',
    'cutting_status': '',
    'cutting_volume_approved': '',
    'cutting_user': '',
    'cutting_method': '',
    'coords': {
        'centroid': [],
        # 'polygon': []
    },
    'raw_response': {},
    'url': None
}

COMMA_LIST = ', , ,'
STRONG_LIST = '<strong> <strong> <strong> <strong>' # 4.times('<strong> ').join() - так не можна Саша? :)
CUT_STATUS = 'Рубка'
CUT_STATUS_STARTED = 'Рубка - Розпочата'
CUT_STATUS_CLOSED = 'Рубка - Закрита'
CUT_STATUS_NOT_STARTED = 'Рубка - Не розпочата'
COL_START = ['datetime', 'ID', 'CODE', 'CODE_TEXT', 'area', 'Extend', 'geometry']
COL_INFO = ['region', 'district', 'quarter', 'square',
            'cutting_ticket_url', 'cutting_status', 'cutting_volume_approved',
            'cutting_user', 'cutting_method']
COL_END = ['url', 'coords',	'raw_response']

"""### Code Functions & Routines

### System Functions
"""

def setup_drive():
  # 1. Authenticate and create the PyDrive client.
  auth.authenticate_user()
  gauth = GoogleAuth()
  gauth.credentials = GoogleCredentials.get_application_default()
  return GoogleDrive(gauth)

def prepare_local_path():
  # choose a local (colab) directory to store the data.
  local_download_path = os.path.expanduser('./')
  try:
    os.makedirs(local_download_path)
  except Exception as err:
    if err.errno != 17:
      print(help(err))
  return local_download_path

def download_files(id_list):
  drive = setup_drive()
  local_download_path = prepare_local_path()

  for id_file in id_list:
    # Create & download by id.
    try:
      downloaded = drive.CreateFile({'id':id_file})
      print('title: {}, id: {}'.format(downloaded['title'], downloaded['id']))
      file_name = os.path.join(local_download_path, downloaded['title'])
      print('downloading to {}'.format(file_name))
      downloaded.GetContentFile(file_name)
    except Exception as err:
      print('[download_files]: file_name={}'.format(id_file), err)

def download_from_ftp(file_list):
  ftp_session = ftplib.FTP('194.44.29.184','ftp_exchange','rSac04jEr2ZuLXp4')

  for file_name in file_list:
    local_file_name = prepare_local_path() + file_name
    print('Trying download {}-file from FTP'.format(file_name))
    ftp_session.retrbinary(
        "RETR " + file_name,
        open(local_file_name, 'wb').write
    )
    print('Downloaded {}-file from FTP to local path: {}'.format(file_name, local_file_name))

  ftp_session.quit()

def make_sub_lists(list_in, chunk_size):
    list_in = iter(list_in)
    return iter(lambda: tuple(islice(list_in, chunk_size)), ())

def make_centroid_sub_lists(gdf, chunk_size):
    centroids = list(
        zip(
            gdf.index,
            zip(
                gdf.centroid.x.values,
                gdf.centroid.y.values)
        )
    )
    return make_sub_lists(centroids, chunk_size)

def custom_draw(x):
  if x['properties']['CODE'] == 1:
    return {'fillColor': '#00ff00', 'fillOpacity': 0.5, 'weight': 2}
  elif x['properties']['CODE'] == 2:
    return {'fillColor': '#ff0000', 'fillOpacity': 0.5, 'weight': 2}
  elif x['properties']['CODE'] == 3:
    return {'fillColor': '#ff69b4', 'fillOpacity': 0.5, 'weight': 2}
  elif x['properties']['CODE'] == 4:
    return {'fillColor': '#0000ff', 'fillOpacity': 0.5, 'weight': 1}
  else:
    return {'fillColor': '#c0c0c0', 'fillOpacity': 0.5, 'weight': 1}

def save_to_postgres(gdf, table_name):
  engine = create_engine(
      "postgresql://deepgreen:deepgreen2021MT!@ec2-15-188-127-126.eu-west-3.compute.amazonaws.com:5432/deepgreen"
  )
  gdf.to_postgis(name=table_name, con=engine, if_exists='replace', index=True)

def get_datetime_from_file(file_for_processing):
  with open(file_for_processing, 'rb') as fd:
    json_file = json.load(fd)

  # raise without excuse :) if any
  return pd.to_datetime(json_file.get('name', '').replace('_', '-'))

"""### Parsing API response and enriching data

#### Parsing and rules identification utils
"""

def parse_response(response, result):
  static_info = [item.strip() for item in COMMA_LIST.split(',')]
  cutting_info = [item.strip() for item in STRONG_LIST.split('<strong>')]
  try:
      response_data = response.get('data', [{}])
      if response_data:
          _static_info = response_data[0].get('static', COMMA_LIST).split(',')
          if _static_info != COMMA_LIST and len(_static_info) == len(static_info):
              static_info = [item.strip() for item in _static_info]

          _cutting_info = response_data[0].get('cutting', STRONG_LIST).split('<strong>')
          if _cutting_info != STRONG_LIST and len(_cutting_info) == len(cutting_info):
              cutting_info = [item.replace('</br>', '').replace('<br>', '').replace('</strong>', '').strip()
                              for item in _cutting_info]
              # if cutting_info[1] != '' and cutting_info[1].find('Закрита') == -1:
              #     print(cutting_info)
  except Exception as err:
      print('Response={} | Error={}'.format(response, err))

  result['region'] = static_info[0].strip()
  result['district'] = static_info[1].strip()
  result['quarter'] = static_info[2].strip()
  result['square'] = static_info[3].strip()

  result['cutting_ticket_url'] = cutting_info[0].strip()
  result['cutting_status'] = cutting_info[1].strip()
  result['cutting_volume_approved'] = cutting_info[2].strip()
  result['cutting_user'] = cutting_info[3].strip()
  result['cutting_method'] = cutting_info[4].strip()

  return result

def set_rules_ver02(result):
  """
    Rules prepared for DEMO-1 by Sash
  """
  if result['region'] != '':
    if CUT_STATUS_STARTED in result['cutting_status']:       # Legal
      result['CODE'] = 1
      result['CODE_TEXT'] = 'Легальна'
    elif CUT_STATUS_CLOSED in result['cutting_status']:      # Not Legal
      result['CODE'] = 2
      result['CODE_TEXT'] = 'Не легальна (Рубка - Закрита)'
    elif CUT_STATUS_NOT_STARTED in result['cutting_status']: # Not Legal | Don't started
      result['CODE'] = 3
      result['CODE_TEXT'] = 'Не легальна (Рубка - Не розпочата)'
    else:
      result['CODE'] = 5                                     # Unknown case
      result['CODE_TEXT'] = 'Невідомий сценарій'
  else:
    result['CODE'] = 4                                     # User is not identified
    result['CODE_TEXT'] = 'Не ідентифікований лісокористувач'

  return result

def set_rules_ver03(result):
  """
    Rules prepared for DEMO-2 by Client&Team
  """
  if result['region'] != '':
    if CUT_STATUS_STARTED in result['cutting_status']:       # Legal
      result['CODE'] = 1
      result['CODE_TEXT'] = 'Легальна діюча'
    elif CUT_STATUS_CLOSED in result['cutting_status']:      # Legal "Closed"
      result['CODE'] = 2
      result['CODE_TEXT'] = 'Легальна закрита'
    elif CUT_STATUS_NOT_STARTED in result['cutting_status']: # Legal "Don't started"
      result['CODE'] = 3
      result['CODE_TEXT'] = 'Легальна не розпочата'
    elif CUT_STATUS in result['cutting_status']:
      result['CODE'] = 6                                     # Unknown case - "Rubka Karl", simply 'Rubka'
      result['CODE_TEXT'] = 'Рубка'
    else:
      result['CODE'] = 4                                     # Not Legal
      result['CODE_TEXT'] = 'Нелегальна відсутній лісорубний'
  else:
    result['CODE'] = 5                                       # Place is not identified
    result['CODE_TEXT'] = 'Проблеми ідентифікації місцерозташування'

  return result

"""#### Main Parsing and Enrichment Flow"""

def send_requests(polygon_flatten, rules_strategy='ver03'):
    result_list = []
    idx = 1
    for idx, point in set(polygon_flatten):
        result = JSON_RESPONSE.copy()
        try:
            result['index'] = idx
            result.setdefault('coords', {})['centroid'] = point
            # result.setdefault('coords', {})['polygon'] = polygon_flatten
            url = URL.format(point[0], point[1])
            result['url'] = url

            response = requests.get(url).json()

            result['raw_response'] = response
            # Defaults
            result = parse_response(response, result)
            # Set the rules
            if rules_strategy.lower() == 'ver02':
              result = set_rules_ver02(result)
            else:
              result = set_rules_ver03(result)
        except Exception as err:
            result.setdefault('raw_response', err)
            print('ERROR', url, result, err)
        idx += 1
        result_list.append(result)
    print('Parsing is finished. Total requests in this session: {}'.format(len(result_list)))
    return pd.DataFrame(result_list).set_index(keys='index')

"""### Main multi-processing crawling flow"""

def iterate_over_centroids(gdf, epsg=3857, limit=5, chunk_size=7):
    num_cpus = psutil.cpu_count(logical=False)

    #print(gdf.crs)
    process_pool = Pool(processes=num_cpus*2)
    start = time.time()

    gdf_new = gdf.to_crs(epsg=epsg)
    print(gdf_new.crs)

    # Start processes in the pool and concat dataframes to one dataframe
    dfs = process_pool.map(
        send_requests,
        [item for item in make_centroid_sub_lists(gdf_new.head(limit), chunk_size)]
      )

    df_processed = pd.concat(
        dfs,
        axis='rows',
        ignore_index=False
    )
    gdf_merged = pd.concat(
        [
            gdf_new.head(limit),
            df_processed
        ],
        axis='columns',
        ignore_index=False
    )

    gdf_merged = gdf_merged.reindex(list(COL_START + COL_INFO + COL_END), axis=1)

    gdf_merged.loc[:, 'CODE'] = gdf_merged.CODE.fillna(5)
    gdf_merged.loc[:, 'CODE'] = gdf_merged.CODE.astype(int)

    gdf_merged.columns = [col.lower() for col in gdf_merged.columns]
    print('Completed in: {} sec(s)'.format(time.time() - start))

    return gdf_merged

"""## Main Flow Execution

---

### 1. Download file from FTP
"""

file_for_processing_1 = '2021_04_05.geojson'
file_for_processing_2 = '2021_04_12.geojson'
download_from_ftp([file_for_processing_1, file_for_processing_2])

# Alternative - download from GDrive for demo purposes
# download_files(
    # [
    #  '1T2YJJwQLHIM5SZtkGjUfgNLFSrbFJ_1m',
    #  '17r3hZkNtuheIAZuXKCnhjHm1L9MHFkUq',
    #  '1KLxqfuwrBWbv65l2EaxlcZptKfZ3zeqb',
    #  '13_FPu_kYd5UngZ486B5hv0D_-7XboM3n'
    #  ]
# )

"""### 2. Read and process downloaded *file*"""

# TEST
# file_datetime = get_datetime_from_file(file_for_processing_2)
# file_datetime

total_requests_to_process = 500

gdf_aug = gpd.read_file(file_for_processing_1).to_crs(3857).tail(total_requests_to_process)
file_datetime_1 = get_datetime_from_file(file_for_processing_1)
gdf_aug.loc[:, 'datetime'] = file_datetime_1

gdf_sep = gpd.read_file(file_for_processing_2).to_crs(3857).tail(total_requests_to_process)
file_datetime_2 = get_datetime_from_file(file_for_processing_2)
gdf_sep.loc[:, 'datetime'] = file_datetime_2

gdf = pd.concat(
    [
      gdf_aug,
      gdf_sep
    ],
    axis='rows',
    ignore_index=True
)
# Normalize the index
gdf.index = pd.Index([i for i in range(1, gdf.shape[0] + 1)])

gdf.head(5)

gdf.datetime.unique()

"""### 3. Start crawling and parsing"""

gdf_enriched = iterate_over_centroids(
    gdf,
    epsg=3857,
    limit=total_requests_to_process,
    chunk_size=25
)

gdf_enriched.info()

gdf_enriched.code.unique()

gdf_enriched[gdf_enriched.code == 3].head(3)

gdf_enriched[['code']].groupby(by='code').size().reset_index(name='counts')

# gdf_enriched.loc[:, 'coords'] = gdf_enriched.coords.apply(lambda x: x.get('centroid'))
# [MultiPolygon([feature]) if type(feature) == Polygon     else feature
# gdf_enriched.loc[:, 'geometry'] = [MultiPolygon(list(x)) for x in gdf_enriched.coords]
from shapely import wkt

gdf_enriched_centroid = gdf_enriched[['code']].copy()
gdf_enriched_centroid.loc[:, 'geometry'] = gdf_enriched.centroid.copy()
gdf_enriched_centroid = gpd.GeoDataFrame(gdf_enriched_centroid, geometry='geometry')
gdf_enriched_centroid.head(3)

"""### 4. Save enriched data to PostgreSQL DB



"""

save_to_postgres(gdf_enriched, table_name='pg_deepgreen_demo_arsen')
print('Data is saved. Please review the table [pg_deepgreen_demo_arsen]')

"""### 5. Visualization"""

# Load extra datasets
# df_point = gpd.read_file('http://www.deepforest.org.ua/geo/point')
# print(df_point.info())
# df_point.head(5)

# df_test = gpd.read_file('http://www.deepforest.org.ua/geo/test')
# print(df_test.info())
# df_test.head()

map_cache_url = 'https://mapa.ukrforest.com/mapcache/'
cutting_url = 'https://mapa.ukrforest.com/wms'

_map = folium.Map(
    # location=[3358337.27, 6503873.82],
    location=[46.168457, 31.331436],
    tiles="cartodbpositron",
    # tiles="OpenStreetMap",
    crs='EPSG3857',
    zoom_start=7,
)

folium.GeoJson(
    gdf_enriched,
    name="Cutting",
    style_function=custom_draw
).add_to(_map)

folium.GeoJson(
    gdf_enriched_centroid,
    name="Cutting-Centroids",
    #style_function=custom_draw
).add_to(_map)

# folium.WmsTileLayer(
#     url=map_cache_url,
#     name='forests',
#     layers='forestries',
#     frm='image/png'
# ).add_to(m)

folium.WmsTileLayer(
    url=map_cache_url,
    name='quarters',
    layers='quarters',
    frm='image/jpeg'
).add_to(_map)

folium.WmsTileLayer(
    url=map_cache_url,
    name='sections',
    layers='sections',
    frm='image/jpeg'
).add_to(_map)

folium.LayerControl().add_to(_map)
_map

# folium.GeoJson(
#     # gdf,#r'./DeepForest_GeoPoint.json',
#     df_test.head(50),
#     name="DeepForest_TEST",
#     style_function=custom_draw
# ).add_to(_map)

# folium.GeoJson(
#     # gdf,#r'./DeepForest_GeoPoint.json',
#     df_point.head(50),
#     name="DeepForest_GeoPoint",
#     style_function=custom_draw
# ).add_to(_map)
